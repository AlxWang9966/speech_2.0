import streamlit as st
from azure.cognitiveservices.speech import SpeechConfig, SpeechRecognizer, AudioConfig
import speech_fast_transcription
import llm_analysis
import realtime_stream  # Real-time (simulated file streaming) transcription
import os
import azure.cognitiveservices.speech as speechsdk  # Azure Speech SDK for live microphone
import queue, time  # Queue for thread-safe event passing; time for periodic rerun
from azure.ai.translation.text import TextTranslationClient  # Live translation client
from azure.core.credentials import AzureKeyCredential

"""
AVIA Main Streamlit App (meeting_sum.py)

Features:
- Batch audio transcription via Azure Fast Transcription API (multi-language autodetect)
- Image upload + multimodal analysis (Azure OpenAI Vision)
- Intelligent multilingual summary generation (Azure OpenAI)
- Simulated continuous transcription & translation (push-stream of uploaded file)
- Live local microphone continuous transcription (Azure Speech SDK) with optional on-the-fly translation

Design Notes:
- Background SDK callbacks MUST NOT touch Streamlit state directly (avoid ScriptRunContext warnings). They enqueue events.
- Main thread drains a Queue each rerun to update session_state (transcription + translations).
- Periodic rerun (st.rerun) every ~1s while live mic is running to keep UI responsive.
- Translation for live mic performed only on finalized segments (NOT partials) for accuracy & cost efficiency.
- Keep this script mostly orchestration; heavy logic lives in helper modules.

TODO (future enhancements): browser-based WebRTC capture, diarization in live mic mode, auto language detection for live mode, export/clear live transcript buttons.
"""

# ===== Page Configuration =====
st.set_page_config(page_title="AVIA | Audio-Visual Intelligence Assistant", page_icon="ğŸ¤–", layout="wide")

# ===== Hero Banner =====
st.markdown("""
<div style="text-align: center; padding: 2rem 0; background: linear-gradient(90deg, #667eea 0%, #764ba2 100%); border-radius: 10px; margin-bottom: 2rem;">
    <h1 style="color: white; font-size: 3rem; margin: 0; text-shadow: 2px 2px 4px rgba(0,0,0,0.3);">ğŸ¤– AVIA</h1>
    <h2 style="color: white; font-size: 1.5rem; margin: 0.5rem 0 0 0; font-weight: 300;">Audio-Visual Intelligence Assistant</h2>
    <p style="color: rgba(255,255,255,0.9); font-size: 1.1rem; margin: 0.5rem 0 0 0;">Multi-language transcription â€¢ Image analysis â€¢ Intelligent summaries</p>
</div>
""", unsafe_allow_html=True)

# ===== Main Content =====
# åˆ›å»ºä¸¤åˆ—å¸ƒå±€
col1, col2 = st.columns(2)

with col1:
    st.markdown("""
    <div style="background: #f8f9fa; padding: 1.5rem; border-radius: 10px; border-left: 4px solid #007bff;">
        <h3 style="color: #007bff; margin-top: 0;">ğŸ¤ Audio Processing</h3>
        <p style="color: #6c757d; margin-bottom: 1rem;">Upload audio files for multi-language transcription and intelligent analysis</p>
    </div>
    """, unsafe_allow_html=True)
    
    audio_file = st.file_uploader(
        "Choose Audio File", 
        type=["wav", "mp3", "m4a"],
        help="Supports: WAV, MP3, M4A formats. Automatic language detection for 7+ languages."
    )
    
    if audio_file:
        st.success("âœ… Audio file uploaded successfully!")
        st.info(f"ğŸ“ File: {audio_file.name} ({audio_file.size} bytes)")
    
    # æ–°å¢å®æ—¶è½¬å½•é€‰é¡¹
    real_time_mode = st.checkbox("Enable simulated continuous transcription + translation", help="Experimental: stream file to Azure Speech and translate segments.")

    # å®æ—¶è½¬å½• - æœ¬åœ°éº¦å…‹é£
    live_exp = st.expander("ğŸ™ï¸ Live Microphone (Local Machine)", expanded=False)
    with live_exp:
        st.caption("Runs only when this app is executed on your own machine with a microphone. Not browser-based streaming.")
        # ----- Session State Initialization (idempotent) -----
        if 'live_segments' not in st.session_state:
            st.session_state.live_segments = []          # Finalized transcript lines
        if 'live_partial' not in st.session_state:
            st.session_state.live_partial = ''           # Current partial hypothesis
        if 'live_running' not in st.session_state:
            st.session_state.live_running = False        # Live mic active flag
        if 'live_recognizer' not in st.session_state:
            st.session_state.live_recognizer = None      # SpeechRecognizer instance
        if 'live_queue' not in st.session_state:
            st.session_state.live_queue = queue.Queue()  # Thread-safe event queue
        if 'live_last_refresh' not in st.session_state:
            st.session_state.live_last_refresh = 0.0      # Timestamp for periodic rerun
        # Translation related state
        if 'live_translate_enabled' not in st.session_state:
            st.session_state.live_translate_enabled = False
        if 'live_target_lang' not in st.session_state:
            st.session_state.live_target_lang = 'zh-CN'
        if 'live_translations' not in st.session_state:
            st.session_state.live_translations = []      # Final translated lines aligned with segments
        if 'live_translator_client' not in st.session_state:
            st.session_state.live_translator_client = None

        # ----- Translation Controls (optional) -----
        st.markdown("---")
        col_cfg_a, col_cfg_b = st.columns([1,1])
        with col_cfg_a:
            st.session_state.live_translate_enabled = st.checkbox("Enable live translation", value=st.session_state.live_translate_enabled, help="Translate finalized segments to target language")
        with col_cfg_b:
            st.session_state.live_target_lang = st.selectbox(
                "Target",
                options=['zh-CN','en-US','ja-JP','ko-KR','fr-FR','de-DE','es-ES'],
                index=0 if st.session_state.live_target_lang not in ['zh-CN','en-US','ja-JP','ko-KR','fr-FR','de-DE','es-ES'] else ['zh-CN','en-US','ja-JP','ko-KR','fr-FR','de-DE','es-ES'].index(st.session_state.live_target_lang),
                disabled=not st.session_state.live_translate_enabled
            )

        # ----- Control Buttons -----
        col_live_a, col_live_b = st.columns(2)
        with col_live_a:
            start_live = st.button("â–¶ï¸ Start Live Mic", disabled=st.session_state.live_running)
        with col_live_b:
            stop_live = st.button("ğŸ›‘ Stop", disabled=not st.session_state.live_running)

        # ----- Start Live Recognition Logic -----
        if start_live and not st.session_state.live_running:
            speech_key = os.getenv('SPEECH_KEY')
            speech_region = os.getenv('SPEECH_REGION', 'eastus')
            if not speech_key:
                st.error("SPEECH_KEY not set in environment (.env).")
            else:
                try:
                    speech_config_live = speechsdk.SpeechConfig(subscription=speech_key, region=speech_region)
                    speech_config_live.set_property(speechsdk.PropertyId.SpeechServiceResponse_PostProcessingOption, "TrueText")
                    audio_config_live = speechsdk.audio.AudioConfig(use_default_microphone=True)
                    # NOTE: Language hard-coded; future enhancement: auto-detect or user selector
                    recognizer_live = speechsdk.SpeechRecognizer(speech_config=speech_config_live, language="en-US", audio_config=audio_config_live)

                    # Initialize translator client if enabled (avoid doing this in callbacks)
                    if st.session_state.live_translate_enabled:
                        t_key = os.getenv('TRANSLATOR_KEY')
                        t_region = os.getenv('TRANSLATOR_REGION') or os.getenv('SPEECH_REGION', 'eastus')
                        t_endpoint = os.getenv('TRANSLATOR_ENDPOINT', 'https://api.cognitive.microsofttranslator.com')
                        if t_key:
                            try:
                                st.session_state.live_translator_client = TextTranslationClient(credential=AzureKeyCredential(t_key), region=t_region, endpoint=t_endpoint)
                            except Exception as te:
                                st.warning(f"Translator init failed: {te}")
                                st.session_state.live_translator_client = None
                        else:
                            st.warning("Translator key not set; translation disabled")
                            st.session_state.live_translate_enabled = False

                    # Capture queue reference (callbacks must not mutate st directly)
                    live_queue_ref = st.session_state.live_queue

                    def _live_recognizing(evt: speechsdk.SpeechRecognitionEventArgs):
                        """Interim hypothesis handler (enqueue partial)."""
                        try:
                            if evt.result.reason == speechsdk.ResultReason.RecognizingSpeech:
                                live_queue_ref.put(('partial', evt.result.text))
                        except Exception:
                            pass

                    def _live_recognized(evt: speechsdk.SpeechRecognitionEventArgs):
                        """Final segment handler (enqueue final)."""
                        try:
                            if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech and evt.result.text:
                                live_queue_ref.put(('final', evt.result.text))
                        except Exception:
                            pass

                    def _live_canceled(evt: speechsdk.SessionEventArgs):  # noqa: D401
                        # Enqueue stop marker for graceful shutdown in main thread
                        try:
                            live_queue_ref.put(('stopped', None))
                        except Exception:
                            pass

                    def _live_stopped(evt):
                        # Session ended normally
                        try:
                            live_queue_ref.put(('stopped', None))
                        except Exception:
                            pass

                    # Wire events
                    recognizer_live.recognizing.connect(_live_recognizing)
                    recognizer_live.recognized.connect(_live_recognized)
                    recognizer_live.canceled.connect(_live_canceled)
                    recognizer_live.session_stopped.connect(_live_stopped)

                    recognizer_live.start_continuous_recognition()
                    st.session_state.live_recognizer = recognizer_live
                    st.session_state.live_running = True
                    st.success("Live microphone transcription started.")
                except Exception as e:
                    st.error(f"Failed to start live recognition: {e}")

        # ----- Drain Queue (Main Thread State Updates) -----
        while st.session_state.get('live_queue') and not st.session_state.live_queue.empty():
            kind, txt = st.session_state.live_queue.get()
            if kind == 'partial':
                st.session_state.live_partial = txt
            elif kind == 'final':
                st.session_state.live_segments.append(txt)
                st.session_state.live_partial = ''
                # Perform translation only for final segments
                if st.session_state.live_translate_enabled and txt:
                    client = st.session_state.live_translator_client
                    if client:
                        try:
                            tr = client.translate(body=[txt], to_language=[st.session_state.live_target_lang])
                            translated_text = tr[0]['translations'][0]['text']
                        except Exception:
                            translated_text = '[Translation failed]'
                    else:
                        translated_text = '[Translator not configured]'
                    st.session_state.live_translations.append(translated_text)
            elif kind == 'stopped':
                st.session_state.live_running = False

        # ----- Stop Logic -----
        if stop_live and st.session_state.live_running and st.session_state.live_recognizer:
            try:
                st.session_state.live_recognizer.stop_continuous_recognition()
                st.session_state.live_running = False
                st.success("Live microphone transcription stopped.")
            except Exception as e:
                st.error(f"Failed to stop recognizer: {e}")

        # ----- Live Output Rendering -----
        if st.session_state.live_running:
            st.info("Microphone active. Speak now...")
        if st.session_state.live_partial:
            st.markdown(f"**Partial:** {st.session_state.live_partial}")
        if st.session_state.live_segments:
            st.markdown("**Final Segments:**")
            st.write("\n".join(st.session_state.live_segments))
            if st.session_state.live_translate_enabled and st.session_state.live_translations:
                st.markdown(f"**Translations ({st.session_state.live_target_lang}):**")
                st.write("\n".join(st.session_state.live_translations))
        if not st.session_state.live_running and not st.session_state.live_segments:
            st.caption("No live transcription yet.")

        # ----- Periodic Auto-Refresh (while streaming) -----
        if st.session_state.live_running:
            now = time.time()
            if now - st.session_state.live_last_refresh > 1.0:
                st.session_state.live_last_refresh = now
                try:
                    st.rerun()
                except AttributeError:  # Older Streamlit fallback
                    st.experimental_rerun()

with col2:
    st.markdown("""
    <div style="background: #f8f9fa; padding: 1.5rem; border-radius: 10px; border-left: 4px solid #28a745;">
        <h3 style="color: #28a745; margin-top: 0;">ğŸ–¼ï¸ Visual Processing</h3>
        <p style="color: #6c757d; margin-bottom: 1rem;">Upload images for content analysis and OCR extraction</p>
    </div>
    """, unsafe_allow_html=True)
    
    image_file = st.file_uploader(
        "Choose Image File", 
        type=["jpg", "jpeg", "png"],
        help="Supports: JPG, JPEG, PNG formats. Analyzes content and extracts text information."
    )
    
    if image_file:
        st.success("âœ… Image file uploaded successfully!")
        st.info(f"ğŸ“ File: {image_file.name} ({image_file.size} bytes)")

# è‡ªå®šä¹‰æç¤ºåŒºåŸŸ
st.markdown("---")
st.markdown("""
<div style="background: #fff3cd; padding: 1rem; border-radius: 8px; border-left: 4px solid #ffc107; margin: 1rem 0;">
    <h4 style="color: #856404; margin-top: 0;">âš™ï¸ Advanced Options</h4>
</div>
""", unsafe_allow_html=True)

user_prompt = st.text_area(
    "Custom Analysis Prompt (Optional)", 
    placeholder="Enter specific instructions for content analysis and summary generation...",
    help="Provide custom instructions to guide the AI analysis. Leave empty to use default intelligent analysis."
)

# å±…ä¸­çš„å¤„ç†æŒ‰é’®
st.markdown("<br>", unsafe_allow_html=True)
col1, col2, col3 = st.columns([1, 2, 1])
with col2:
    process_button = st.button(
        "ğŸš€ Process with AVIA", 
        use_container_width=True,
        disabled=not (audio_file or image_file)
    )

if process_button and (audio_file or image_file):
    transcription = None
    image_result = None
    detected_language = "en-US"  # Default language
    
    # å¤„ç†è¿›åº¦æŒ‡ç¤ºå™¨
    st.markdown("---")
    st.markdown("""
    <div style="text-align: center; margin: 2rem 0;">
        <h3 style="color: #495057;">ğŸ”„ AVIA Processing...</h3>
        <p style="color: #6c757d;">Analyzing your content with advanced AI capabilities</p>
    </div>
    """, unsafe_allow_html=True)
    
    with st.spinner("Processing with AVIA intelligence..."):
        # è½¬å½•éŸ³é¢‘
        if audio_file:
            st.info("ğŸ¤ Processing audio file...")
            result, detected_lang = speech_fast_transcription.fast_transcript(audio_file)
            
            if result:
                transcription = result
                detected_language = detected_lang if detected_lang else "en-US"
                st.success(f"âœ… Audio transcription completed! Language detected: {detected_language}")
            else:
                transcription = "Audio transcription failed."
                st.error("âŒ Audio transcription failed")

        # åˆ†æå›¾åƒå†…å®¹
        if image_file:
            st.info("ğŸ–¼ï¸ Processing image file...")
            image_result = llm_analysis.analysis_image(image_file, detected_language)
            if image_result:
                st.success("âœ… Image analysis completed!")
            else:
                st.error("âŒ Image analysis failed")
        

        # æ€»ç»“å†…å®¹ - Create language-adaptive prompt
        st.info("ğŸ¤– Generating intelligent summary...")
        language_prompts = {
            "en-US": f"""
        Audio transcription: {transcription}
        Image analysis: {image_result}

        Please provide a comprehensive summary focusing on key content and important information.
        """,
            "zh-CN": f"""
        éŸ³é¢‘è½¬å½•ï¼š{transcription}
        å›¾åƒåˆ†æï¼š{image_result}

        è¯·æä¾›ä¸€ä»½ä¾§é‡äºå…³é”®å†…å®¹å’Œé‡è¦ä¿¡æ¯çš„æ€»ç»“ã€‚
        """,
            "es-ES": f"""
        TranscripciÃ³n de audio: {transcription}
        AnÃ¡lisis de imagen: {image_result}

        Por favor, proporciona un resumen completo enfocÃ¡ndose en el contenido clave y la informaciÃ³n importante.
        """,
            "fr-FR": f"""
        Transcription audio: {transcription}
        Analyse d'image: {image_result}

        Veuillez fournir un rÃ©sumÃ© complet en vous concentrant sur le contenu clÃ© et les informations importantes.
        """,
            "de-DE": f"""
        Audio-Transkription: {transcription}
        Bildanalyse: {image_result}

        Bitte erstellen Sie eine umfassende Zusammenfassung mit Fokus auf wichtige Inhalteå’Œä¿¡æ¯ã€‚
        """,
            "ja-JP": f"""
        éŸ³å£°è»¢å†™: {transcription}
        ç”»åƒåˆ†æ: {image_result}

        é‡è¦ãªå†…å®¹ã¨æƒ…å ±ã«ç„¦ç‚¹ã‚’å½“ã¦ãŸåŒ…æ‹¬çš„ãªè¦ç´„ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
        """,
            "ko-KR": f"""
        ì˜¤ë””ì˜¤ ì „ì‚¬: {transcription}
        ì´ë¯¸ì§€ ë¶„ì„: {image_result}

        ì£¼ìš” ë‚´ìš©ê³¼ ì¤‘ìš”í•œ ì •ë³´ì— ì¤‘ì ì„ ë‘” í¬ê´„ì ì¸ ìš”ì•½ì„ ì œê³µí•´ ì£¼ì„¸ìš”.
        """
        }
        
        # Use detected language or default to English
        summary_prompt = language_prompts.get(detected_language, language_prompts["en-US"])

        summary = llm_analysis.analysis_text(user_prompt, summary_prompt, detected_language)
        st.success("âœ… Summary generation completed!")
       


    # Language-adaptive UI labels
    ui_labels = {
        "en-US": {
            "transcription_header": "ğŸ“ Audio Transcription",
            "download_transcription": "Download Audio Transcription",
            "summary_header": "ğŸ“ Intelligent Summary"
        },
        "zh-CN": {
            "transcription_header": "ğŸ“ éŸ³é¢‘è½¬å½•",
            "download_transcription": "ä¸‹è½½éŸ³é¢‘è½¬å½•",
            "summary_header": "ğŸ“ æ™ºèƒ½æ‘˜è¦"
        },
        "es-ES": {
            "transcription_header": "ğŸ“ TranscripciÃ³n de Audio",
            "download_transcription": "Descargar TranscripciÃ³n de Audio",
            "summary_header": "ğŸ“ Resumen Inteligente"
        },
        "fr-FR": {
            "transcription_header": "ğŸ“ Transcription Audio",
            "download_transcription": "TÃ©lÃ©charger la Transcription Audio",
            "summary_header": "ğŸ“ RÃ©sumÃ© Intelligent"
        },
        "de-DE": {
            "transcription_header": "ğŸ“ Audio-Transkription",
            "download_transcription": "Audio-Transkription Herunterladen",
            "summary_header": "ğŸ“ Intelligente Zusammenfassung"
        },
        "ja-JP": {
            "transcription_header": "ğŸ“ éŸ³å£°è»¢å†™",
            "download_transcription": "éŸ³å£°è»¢å†™ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            "summary_header": "ğŸ“ ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆè¦ç´„"
        },
        "ko-KR": {
            "transcription_header": "ğŸ“ ì˜¤ë””ì˜¤ ì „ì‚¬",
            "download_transcription": "ì˜¤ë””ì˜¤ ì „ì‚¬ ë‹¤ìš´ë¡œë“œ",
            "summary_header": "ğŸ“ ì§€ëŠ¥í˜• ìš”ì•½"
        }
    }
    
    labels = ui_labels.get(detected_language, ui_labels["en-US"])

    # åˆ›å»ºç»“æœå±•ç¤ºåŒºåŸŸ
    st.markdown("---")
    st.markdown("""
    <div style="text-align: center; margin: 2rem 0;">
        <h2 style="color: #495057;">ğŸ“Š AVIA Analysis Results</h2>
        <p style="color: #6c757d;">Your content has been processed and analyzed</p>
    </div>
    """, unsafe_allow_html=True)

    # ç»“æœå¸ƒå±€
    if transcription and image_result:
        # ä¸¤åˆ—å¸ƒå±€ï¼šè½¬å½•å’Œå›¾åƒåˆ†æ
        result_col1, result_col2 = st.columns(2)
        
        with result_col1:
            st.subheader(labels["transcription_header"])
            st.download_button(
                label=labels["download_transcription"],
                data=transcription,
                file_name="avia_transcription.txt",
                mime="text/plain",
                use_container_width=True
            )
        
        with result_col2:
            st.markdown("""
            <div style="background: #f0f8e7; padding: 1rem; border-radius: 8px; border-left: 4px solid #28a745;">
            """, unsafe_allow_html=True)
            st.subheader("ğŸ–¼ï¸ Image Analysis")
            if image_result:
                with st.expander("View Image Analysis Details", expanded=False):
                    st.write(image_result)
            st.markdown("</div>", unsafe_allow_html=True)
    
    elif transcription:
        # åªæœ‰éŸ³é¢‘
        st.subheader(labels["transcription_header"])
        st.download_button(
            label=labels["download_transcription"],
            data=transcription,
            file_name="avia_transcription.txt",
            mime="text/plain",
            use_container_width=True
        )
    
    elif image_result:
        # åªæœ‰å›¾åƒ
        st.markdown("""
        <div style="background: #f0f8e7; padding: 1.5rem; border-radius: 8px; border-left: 4px solid #28a745;">
        """, unsafe_allow_html=True)
        st.subheader("ğŸ–¼ï¸ Image Analysis")
        with st.expander("View Image Analysis Details", expanded=True):
            st.write(image_result)
        st.markdown("</div>", unsafe_allow_html=True)

    # ä¸»è¦æ‘˜è¦åŒºåŸŸ
    st.markdown("<br>", unsafe_allow_html=True)
    st.markdown("""
    <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 1.5rem; border-radius: 10px; margin: 1rem 0;">
        <h3 style="color: white; margin-top: 0; text-align: center;">âœ¨ AVIA Intelligent Summary</h3>
    </div>
    """, unsafe_allow_html=True)
    
    # Summary content - just display the summary text
    st.write(summary)
    
    # Simple download button for the summary
    st.download_button(
        label="ï¿½ Download Summary",
        data=summary,
        file_name="avia_summary.txt",
        mime="text/plain",
        use_container_width=True,
        help="Download summary as a text file"
    )

# å®æ—¶è½¬å½•å’Œç¿»è¯‘å¤„ç† - æ–°å¢é€»è¾‘
if audio_file and real_time_mode:
    st.info("ğŸ›°ï¸ Starting simulated continuous transcription & translation...")
    with st.spinner("Streaming & processing..."):
        rt_result = realtime_stream.continuous_transcribe_and_translate(audio_file, source_language="en-US", target_language="zh-CN")
    if rt_result.error:
        st.error(f"Real-time module error: {rt_result.error}")
    else:
        st.success("âœ… Continuous session completed")
        with st.expander("Real-time Segments", expanded=False):
            if rt_result.final_segments:
                st.markdown("**Original Segments:**")
                st.write("\n".join(rt_result.final_segments))
            if rt_result.translated_segments:
                st.markdown("**Translated Segments:**")
                st.write("\n".join(rt_result.translated_segments))
